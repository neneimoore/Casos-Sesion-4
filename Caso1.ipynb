{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Mantenimiento proactivo: predicción de fallas con ML\n","\n","Una empresa tiene una flota de dispositivos que transmiten lecturas diarias de sensores. Les gustaría crear una solución de mantenimiento predictivo para identificar de forma proactiva cuándo se debe realizar el mantenimiento. Este enfoque promete ahorros de costos con respecto al mantenimiento preventivo de rutina o basado en el tiempo, porque las tareas se realizan sólo cuando están justificadas.\n","\n","El objetivo consiste en construir un modelo predictivo utilizando el aprendizaje automático para predecir la probabilidad de que falle un dispositivo. Al construir este modelo, asegúrese de minimizar los falsos positivos y los falsos negativos. La columna que está intentando predecir se llama falla con valor binario 0 para no falla y 1 para falla.\n","\n","\n","Fuente: https://www.kaggle.com/code/ahmettalhabektas/proactive-maintenance-predicting-failures-with-ml"],"metadata":{"id":"e4m3EG9NqAZA"}},{"cell_type":"markdown","source":["# 1. Cragar la librerías necesarias"],"metadata":{"id":"vOG_wDm6ij5_"}},{"cell_type":"code","source":["##!pip install pandas-profiling -q"],"metadata":{"id":"eKUpggQJik91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ydata-profiling -q"],"metadata":{"id":"f_IOf2aIoMTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ydata_profiling import ProfileReport"],"metadata":{"id":"OTr793H8oQHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMAprzv-xfHM"},"outputs":[],"source":["## Pandas para manipular datos\n","import pandas as pd\n","\n","## Matplotlib para graficar resultados\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","## Seaborn para visualización de datos\n","import seaborn as sns\n","## Numpy para las operaciones numéricas\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","warnings.simplefilter('ignore')\n","\n","\n","## Importando los algoritmos de ML supervisado\n","from sklearn.naive_bayes import GaussianNB, BernoulliNB  # For binary classification\n","##from sklearn.naive_bayes import MultinomialNB  # For multi-class classification\n","from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors classifier\n","## Support Vector Machines\n","from sklearn.svm import SVC\n","## Árbol de decisión\n","from sklearn.tree import DecisionTreeClassifier\n","## Regresión logística\n","from sklearn.linear_model import LogisticRegression\n","##from sklearn.ensemble import GradientBoostingClassifier  # Gradient Boosting classifier\n","\n","## Métricas para evaluación de modelos\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","## Herramienta para seprar datos de entrenamiento y testeo (validación)\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","source":["# 2. Lectura de datos"],"metadata":{"id":"iSuz30iCryvX"}},{"cell_type":"code","source":["## Definir acceso al Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"HfAkngBwmn8I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Define la ruta de acceso y archivo con datos\n","path = '/content/drive/MyDrive/AA - Salfa/Casos/'\n","filename = 'DatosFallas-V1.xlsx'\n","df = pd.read_excel(path+filename)"],"metadata":{"id":"LDedmxaMytEG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Análisis Exploratorio de Datos (EDA)\n","\n","Entender el comportamiento de cada variable individualmente calculando recuentos de frecuencia, visualizando las distribuciones, etc. También las relaciones entre las diversas combinaciones de las variables predictoras y de respuesta creando diagramas de dispersión, correlaciones, etc.\n","\n","- Comprender qué variables podrían ser importantes para predecir la Y (respuesta);\n","- Generar insights que nos permitan una mayor comprensión del contexto y desempeño del negocio.\n","\n","EDA suele ser parte de cada proyecto de aprendizaje automático/modelado predictivo, especialmente con conjuntos de datos tabulares."],"metadata":{"id":"ZXlZ50Uay0KV"}},{"cell_type":"code","source":["df.profile_report()"],"metadata":{"id":"JI785txFywtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Entrega la cantida de filas y columnas\n","df.shape"],"metadata":{"id":"dBTufHGyzAP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Borra las filas duplicadas\n","df.drop_duplicates(inplace=True)\n","df.shape"],"metadata":{"id":"wNJYdKadzBho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Grafica dos métricas\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df['Metrica 7'], df['Metrica 8'], alpha=0.5)\n","plt.title('Gráfico entre Metrica 7 y Metrica 8')\n","plt.xlabel('Metrica 7')\n","plt.ylabel('Metrica 8')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"hmPzZhH8zE5a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Al parecer hay muchos datos concentrados al inición de los valores, lo cual genera distribuciones cargadas hacia los valores iniciales.\n","\n","Entonces, se rectifican los valores realizando una transformación logoritmica de ellos"],"metadata":{"id":"FcUYl2ODyZ5W"}},{"cell_type":"code","source":["for num in [\"2\",\"3\",\"4\",\"7\",\"8\",\"9\"]:\n","    df[f'Metrica {num}'] = np.log1p(df[f'Metrica {num}'])"],"metadata":{"id":"4zZJ6xWVzIaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Gráfico de dos métricas\n","plt.figure(figsize=(10, 6))\n","plt.scatter(df['Metrica 7'], df['Metrica 8'], alpha=0.5)\n","plt.title('Gráfico entre Metrica 7 y Metrica 8')\n","plt.xlabel('Metrica 7')\n","plt.ylabel('Metrica 8')\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"XcHLs-Q_zL07"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hay una fuerte correlación entre la métrica 7 y 8, por lo cual no agrega ninguna información manejar las dos. Se procede a eliminar la metrica 8"],"metadata":{"id":"zgEqUYR_yxmn"}},{"cell_type":"code","source":["df.drop(\"Metrica 8\", axis = 1, inplace = True)"],"metadata":{"id":"L6LKjQGlzPNa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Se resumen los datos disponibles"],"metadata":{"id":"zt_G3FeMzB66"}},{"cell_type":"code","source":["def summarize_data(df):\n","    print(\"Número de filas y columnas:\", df.shape)\n","    print(\"\\nColumnas de datos:\", df.columns)\n","    print(\"\\nTipo de datos y valores perdidos:\")\n","    print(df.info())\n","    print(\"\\nResumen de las columnas numéricas:\")\n","    print(df.describe())\n","    print(\"\\nValores perdidos:\")\n","    print(df.isnull().sum())\n","    print(\"\\nValores únicos en la columna 'failure':\")\n","    print(df['Falla'].value_counts())\n","\n","## Llama la función de resumen\n","summarize_data(df)"],"metadata":{"id":"RKKVbaMozSP5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Revisamos cuántos dispositivos estamos monitoreando, primero que nos entregue la columna de los nombres de los dispositivos"],"metadata":{"id":"BT_leCTtz3WI"}},{"cell_type":"code","source":["df[\"Dispositivo\"]"],"metadata":{"id":"EQTgd9nmzYvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"Ca49T23F19ms"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora los dispositivos diferentes existentes en el monitoreo"],"metadata":{"id":"adWN_0RO0G7h"}},{"cell_type":"markdown","source":["**Nueva Características**\n","\n","Creamos la características llamada 'device_model' que contiene los diferentes dispositivos (4 primeros caractéres) existentes en el monitoreo, y nos muestra la cantidad de registros asociados"],"metadata":{"id":"X_uGxQ3o0Lx8"}},{"cell_type":"code","source":["df[\"Modelo_Dispositivo\"] = df[\"Dispositivo\"].apply(lambda x : x[:4])\n","df[\"Modelo_Dispositivo\"].value_counts()"],"metadata":{"id":"Iv5hU7HrrRdT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creamos otra variable con los cuatro segundos caractéres de los nombre de los dispositivos"],"metadata":{"id":"Fn9X71QH2YAv"}},{"cell_type":"code","source":["df[\"Resto_Dispositivo\"] = df[\"Dispositivo\"].apply(lambda x : x[4:])\n","df[\"Resto_Dispositivo\"].value_counts()[:20]"],"metadata":{"id":"AaLFYjmfzd8F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ahora eliminamos la columna 'device' ya que fue separada en dos diferentes ('device_model'y 'device_rest')"],"metadata":{"id":"tWIOaNhc2pHb"}},{"cell_type":"code","source":["df.drop(\"Dispositivo\",axis=1,inplace=True)"],"metadata":{"id":"pm6FGiY9zk84"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"_1KQMdSR23Nn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Histogramas de fallas y no fallas**\n","\n","Construimos un histograma de la cantidad de fallas y no fallas por dispositivo"],"metadata":{"id":"-iCMrF1G26Pa"}},{"cell_type":"code","source":["## Creamos dos gráficos\n","plt.figure(figsize=(12, 6))\n","## Graficamos la distribución (histograma) de las fallas por dispositivos (failure=1)\n","plt.subplot(1, 2, 1)\n","sns.countplot(x='Modelo_Dispositivo', data=df.loc[df['Falla'] == 1])\n","plt.title('Distribución de falla (Falla=1) por dispositivo')\n","\n","## Graficamos la distribución de la no falla (failure=0) por dispositivo\n","plt.subplot(1, 2, 2)\n","sns.countplot(x=\"Modelo_Dispositivo\", data=df.loc[df[\"Falla\"] == 0])\n","plt.title('Distribución de no falla (Falla=0) por dispositivo')\n","\n","## Ajustamos el layout\n","plt.tight_layout()\n","\n","## Mostramos los gráficos\n","plt.show()"],"metadata":{"id":"UisbuvzDzmIG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Al parecer el dispositivo Z1F2 presentan sólo casos de no falla, lo cual no nos permite generar la clasificación en forma adecuada (no tiene datos de fallas).\n","\n","Entonces, decidimos eliminar las filas asociadas a dicho dispositivo"],"metadata":{"id":"LER4S5Yz4dzZ"}},{"cell_type":"code","source":["df.drop(df.loc[df[\"Modelo_Dispositivo\"]==\"Z1F2\"].index,axis=0,inplace=True)\n","df.reset_index(drop=True,inplace=True)\n","df.tail()"],"metadata":{"id":"n-ePde14zpgL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note que de los 124.494 datos originales (filas), debido a la eliminacón de duplicados y ahora de dispositivos sin fallas sólo quedan 124.242 filas"],"metadata":{"id":"eu7ceRqI44Af"}},{"cell_type":"markdown","source":["**Entendiendo device_rest**\n","\n","Ahora queremos saber la distribución asociada a la característica devide_rest, entonces graficamos para el caso de fallas solamente (Falla=1)"],"metadata":{"id":"KMxjYXFu5W9K"}},{"cell_type":"code","source":["## Crea el gráfico para este análisis\n","plt.figure(figsize=(12, 6))\n","sns.countplot(x=\"Resto_Dispositivo\", data=df.loc[df[\"Falla\"] == 1])\n","plt.title('Distribución de la no falla (Falla=1) con respecto al dispositivo')\n","\n","plt.tight_layout()\n","\n","plt.show()"],"metadata":{"id":"EGR0p0Xhzsa0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La verdad no entrega mayor información, ya que no presenta una cantidad de más de un dato en cuento a las fallas. Por esta razón, decidimos eliminar esta variable"],"metadata":{"id":"5RQNXINz5_MF"}},{"cell_type":"code","source":["df.drop(\"Resto_Dispositivo\", axis = 1, inplace = True)\n","df.sample(5)"],"metadata":{"id":"m3BTnu49zvlU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Histograma de las diferentes características"],"metadata":{"id":"crF6wcs06VRr"}},{"cell_type":"code","source":["## Crea el histograma de las diferentes métricas\n","## considera el caso de las no falla (failure = 0)\n","plt.figure(figsize=(4*5, 2*5))\n","print(\"Distribución para no falla (Falla = 0)\")\n","mask = df.Falla == 0\n","for i, col in enumerate(['Metrica 1', 'Metrica 2', 'Metrica 3', 'Metrica 4', 'Metrica 5', 'Metrica 6', 'Metrica 7', 'Metrica 9']):\n","    plt.subplot(2, 4, i + 1)\n","    sns.histplot(data=df.loc[mask], x=col, kde=True)\n","    plt.title(f'Distribución de {col}')\n","plt.tight_layout()"],"metadata":{"id":"Gu_LDsN8zzBI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Ahora las distribuciones para falla (failure=1)\n","plt.figure(figsize=(20, 10))\n","print(\"Distribución para fallas (failure = 1)\")\n","mask = df.Falla > 0\n","for i, col in enumerate(['Metrica 1', 'Metrica 2', 'Metrica 3', 'Metrica 4', 'Metrica 5', 'Metrica 6', 'Metrica 7', 'Metrica 9']):\n","    plt.subplot(2, 4, i + 1)\n","    sns.histplot(data=df.loc[mask], x=col, kde=True)\n","    plt.title(f'Distribución de {col}')\n","plt.tight_layout()"],"metadata":{"id":"E92sRX4Xz2-t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Manejo de fechas**\n","\n","Se transforma el formato de las fechas a uno que entregue formato manejable"],"metadata":{"id":"izlWnARB7aRB"}},{"cell_type":"code","source":["## Convierte 'date' a formato datetime\n","df['Fecha'] = pd.to_datetime(df['Fecha'])\n","\n","## Extrae y formatea el mes para graficar\n","df['mes'] = df['Fecha'].dt.to_period('M')\n","df['mes'] = df['mes'].dt.strftime('%Y-%m')\n","\n","## Crea un gráfico de línea para visualizar 'failure' sobre los meses\n","plt.figure(figsize=(10, 6))\n","sns.lineplot(data=df, x='mes', y='Falla')\n","plt.xticks(rotation=45)\n","plt.title(\"Falla en el tiempo (meses)\")"],"metadata":{"id":"MC4vqLMhz6Iw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Extrae y formatea la columna 'week'para graficar\n","df['semana'] = df['Fecha'].dt.to_period('W')\n","df['semana'] = df['semana'].dt.strftime('%Y-%U')\n","\n","## Grafica las fallas por semana\n","plt.figure(figsize=(10, 6))\n","sns.lineplot(data=df, x='semana', y='Falla')\n","plt.xticks(rotation=45)\n","plt.title(\"Fallas por semana\")"],"metadata":{"id":"xRW-6sO4z-hs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mostramos los nuevos datos que incluyen una columna o característica 'mes' y 'semana'"],"metadata":{"id":"_upYvblWxbi3"}},{"cell_type":"code","source":["df"],"metadata":{"id":"1rJk8x90xdOC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Correlaciones entre datos\n","\n","Sólo para aquellas columnas o características numéricas se calcula las correlaciones cruzadas existentes"],"metadata":{"id":"Fq5GJkCq7hXx"}},{"cell_type":"code","source":["## Seleccionamos las columnas numéricas para la matriz de correlaciones\n","numeric_cols = df.select_dtypes(include=[np.number])\n","\n","## Calcula la matriz de correlación\n","correlation_matrix = numeric_cols.corr()\n","\n","## Crea un mapa de calor para visualizar la matriz de correlación\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n","plt.title(\"Matriz de Correlación\")"],"metadata":{"id":"WXecQ57v0B97"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dado que la falla es generalmente cero (no falla), buscar la correlación no sería eficiente"],"metadata":{"id":"-mdm1Ny-sUOa"}},{"cell_type":"code","source":["## Calculamos un gráfico de frecuencia de falla y no falla\n","plt.figure(figsize=(6, 4))\n","sns.countplot(data=df, x='Falla')\n","plt.title(\"Distribución de falla\")"],"metadata":{"id":"EH-RtRa_0F3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como es posible observar desde el gráfico la cantidad de no falla son muy superior a la cantidad de fallas en los datos. Esta situación de ser utilizada directamente traerí un problema de sub muestreo para el caso de las fallas.\n","\n","Es razonable seguir con los datos tal cual como están?, para que situación sería razonable?\n"],"metadata":{"id":"RRfT4ByfsX-u"}},{"cell_type":"markdown","source":["## Estudio de fallas\n","\n","Queremos revisar si las fallas se dan en días específicos de las semana, fin de semana o días del mes. Para ello, creamos columnas o características adicionales que incluyan el día de la semana, el día del mes y si es fin de semana"],"metadata":{"id":"EBx2f0KfzgHD"}},{"cell_type":"code","source":["## Obtenemos el día de la semana, día del mes y si es fin de semana desde\n","## la columna fecha\n","df['Fecha'] = pd.to_datetime(df['Fecha'])\n","df['dia_semana'] = df['Fecha'].dt.dayofweek\n","df['dia_mes'] = df['Fecha'].dt.day\n","df['fin_semana'] = df['dia_semana'].apply(lambda x: 1 if x >= 5 else 0)"],"metadata":{"id":"Rli7VHO60JKT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Ahora creamos gráficos para visualizar las distribuciones de falla\n","## por día de la semana, día del mes y fin de semana\n","plt.figure(figsize=(15, 5))\n","\n","## Distribución por día de la semana\n","plt.subplot(131)\n","sns.countplot(data=df, x='dia_semana', palette='Set3')\n","plt.title(\"Distribución por día de semana\")\n","plt.xlabel(\"Día de semana\")\n","plt.ylabel(\"Cantidad\")\n","\n","## Distribución por día del mes\n","plt.subplot(132)\n","sns.countplot(data=df, x='dia_mes', palette='Set3')\n","plt.title(\"Distribución por día del mes\")\n","plt.xlabel(\"Día del mes\")\n","plt.ylabel(\"Cantidad\")\n","\n","## Distribución por fin de semana\n","plt.subplot(133)\n","sns.countplot(data=df, x='fin_semana', palette='Set3')\n","plt.title(\"Distribución por fin de semana\")\n","plt.xlabel(\"Fin de semana (1) or Día de semana (0)\")\n","plt.ylabel(\"Cantidad\")\n","\n","## Muestra los gráficos\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"CykJG7pS0MdN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Extraemos el número del mes y número de la semana y eliminamos 'date'\n","df['mes'] = df['Fecha'].dt.month\n","df['semana'] = df['Fecha'].dt.isocalendar().week\n","df = df.drop(['Fecha'], axis=1)\n","\n","## Chequeamos los datos del nuevo dataframe\n","df.info()"],"metadata":{"id":"POpg50RF0VzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Para aplicar la codificación One Hot Encoding se utiliza la función pd.get_dummies()\n","## Cada variable se convierte en tantas variables 0/1 como valores diferentes haya.\n","## Las columnas de la salida reciben el nombre de un valor; si la entrada es un DataFrame,\n","## el nombre de la variable original se antepone al valor\n","## La opción drop_first indica que se deben sacar los dummies k-1 de los k niveles\n","## categóricos eliminando el primer nivel\n","df = pd.get_dummies(df,drop_first=True)\n","df"],"metadata":{"id":"7FERyXBs0Y-c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extración de valores aleatorios de datos\n","\n","Debido al fuerte desbalance entre no fallas (124.388) y fallas (106) originalmente que se observó anteriormente, entonces muestreamos desde las filas de no falla un subconjunto que nos permita trabajar sin sesgo."],"metadata":{"id":"dDI7NaigG858"}},{"cell_type":"code","source":["## Importanmos una librería de submuestreo aleatorio\n","from imblearn.under_sampling import RandomUnderSampler"],"metadata":{"id":"6AAZFYTr0cai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Crea una copia del dataframe 'df'\n","X = df.copy()\n","\n","## Crea una copia de la variable 'Falla\" en 'Y'\n","Y = df[\"Falla\"]\n","\n","## Elimina desde la copia 'X' la columna 'Falla'\n","X.drop(\"Falla\", axis=1, inplace=True)"],"metadata":{"id":"Y_Oxipmw0f5n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Creamos una instancia de RandomUnderSampler con un valor para random state\n","rus = RandomUnderSampler(random_state=42)\n","\n","## Realizamos el muestreo under-sampling y obtenemos las filas resampled de\n","## características 'X' y variable objetivo 'Y'\n","X_resampled, y_resampled = rus.fit_resample(X, Y)"],"metadata":{"id":"8ja9g5qk0jAT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Creamos un nuevo dataframe 'under_sample' copiando las muestras resampled\n","## de las características y agregamos la columna 'failure'\n","under_sample = X_resampled.copy()\n","under_sample[\"failure\"] = y_resampled"],"metadata":{"id":"mpT07Pir0npT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Mostramos el nuevo data frame desde los datos under-sampled\n","under_sample.sample(10)"],"metadata":{"id":"WpVPz5NN0o3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["under_sample.shape"],"metadata":{"id":"1CcT007TJllq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notamos que la cantidad de datos disponibles para el trabajo de modelos de ML supervisados es de sólo 212 filas, pero esto debería dar una muestra equilibrada de los datos"],"metadata":{"id":"m5vKXbOJJ0sT"}},{"cell_type":"code","source":["## Finalmente, graficamos las no fallas y fallas en un gráfico de frecuencia\n","## desde el nuevo dataframe under-sampled\n","plt.figure(figsize=(6, 4))\n","sns.countplot(data=under_sample, x='failure')\n","plt.title(\"Distribución de las fallas\")"],"metadata":{"id":"6oJlFII70r6E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Observamos ahora una muestra equitativa entre las no fallas y fallas"],"metadata":{"id":"KWpNzE2yJakG"}},{"cell_type":"markdown","source":["# Modelo de ML\n","\n","Dado que tenemos los datos que nos permiten construir modelos de ML, entonces procedemos a construirlos.\n","\n","Desde los datos (under_sample) lo que debemos hacer es separar los datos en aquellos que se utilizarán para el entrenamiento de los diversos modelos (x_train e y_train) que representarán un 20% de todos los datos disponibles (212 filas), y aquellos de validación o testeo (x_test e y_test). Para tales efectos se utiliza la librería train_test_split.\n","\n","En forma adicional, y de manera de eliminar los efectos de escalas diferentes de los datos, se utiliza un\"escalador\" que tiene por objetivo escalar los datos de manera de hacerlos comparables en escala. Para tales efectos se utiliza la libería StandarScaler"],"metadata":{"id":"SV3ENHRKIJsY"}},{"cell_type":"code","source":["## Importamos las librería necesarias para crear los datos de los modelos\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","## Separamos los datos en 'caracteríasticas'y 'variable a explicar'\n","X_norm = under_sample.drop(['failure'], axis=1)\n","y_norm = under_sample['failure']\n","\n","## Separamos los datos en entrenamiento (train) y validación (test)\n","x_train, x_test, y_train, y_test = train_test_split(X_norm, y_norm, test_size=0.2, random_state=42)\n","\n","## Ahora, realizamos el escalamiento de los datos de entrenamiento y test\n","scaler = StandardScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.transform(x_test)"],"metadata":{"id":"NMsxxx4z0vUZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oNw5oDpHg9di"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Importamos la librería para los modelos**"],"metadata":{"id":"RAh-m5H-Mz6e"}},{"cell_type":"code","source":["## Importamos las librerías de los modelo de clasificación de ML\n","from sklearn.naive_bayes import GaussianNB, BernoulliNB\n","#######from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression #####, SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier #####, GradientBoostingClassifier, , AdaBoostClassifier, ExtraTreesClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"],"metadata":{"id":"TZ0iXUZj0zGC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Creamos los modelos**"],"metadata":{"id":"bXtpKh1RM_JB"}},{"cell_type":"code","source":["## Define una función con diferentes clasificadores y retorna\n","## un dataframe con las métricas de evaluación\n","def evaluate_model(x_train, y_train, x_test, y_test):\n","    ## Lista de clasificadores a utilizar\n","    classifiers = [\n","        RandomForestClassifier(),\n","        DecisionTreeClassifier(),\n","        KNeighborsClassifier(),\n","        GaussianNB(),\n","        BernoulliNB(),\n","        SVC(),\n","        LogisticRegression(),\n","    ]\n","\n","    ## Define los nombre de los clasificadores\n","    classifier_names = [\n","        'RandomForest',\n","        'DecisionTree',\n","        'KNeighbors',\n","        'GaussianNB',\n","        'BernoulliNB',\n","        'SVC',\n","        'LogisticRegression',\n","    ]\n","    ## Crea una dataframe vacío para las métricas\n","    metrics = pd.DataFrame(columns=['Accuracy', 'Precision', 'Recall', 'F1'], index=classifier_names)\n","\n","    ## Evalúa cada clasificador y almacena las métricas\n","    for i, clf in enumerate(classifiers):\n","        clf.fit(x_train, y_train)\n","        y_pred = clf.predict(x_test)\n","\n","        accuracy = accuracy_score(y_test, y_pred)\n","        precision = precision_score(y_test, y_pred)\n","        recall = recall_score(y_test, y_pred)\n","        f1 = f1_score(y_test, y_pred)\n","\n","        metrics.loc[classifier_names[i], 'Accuracy'] = accuracy\n","        metrics.loc[classifier_names[i], 'Precision'] = precision\n","        metrics.loc[classifier_names[i], 'Recall'] = recall\n","        metrics.loc[classifier_names[i], 'F1'] = f1\n","\n","    ## Ordena las métricas por exactitud (accuracy) en orden descendente\n","    metrics = metrics.sort_values(by='Accuracy', ascending=False)\n","\n","    return metrics"],"metadata":{"id":"y9lJcuhzM9QC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Paso 4: Llama a la función para evaluar los modelos\n","metrics = evaluate_model(x_train, y_train, x_test, y_test)"],"metadata":{"id":"AD9n6j5sOVWG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics"],"metadata":{"id":"EfhM1u5n1Hwc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Instalamos optuna\n","\n","Optuna es un marco de software de optimización automática de hiperparámetros, especialmente diseñado para el aprendizaje automático\n","\n","Referencia: https://optuna.readthedocs.io/en/stable/"],"metadata":{"id":"bPzeye1gPAcU"}},{"cell_type":"code","source":["!pip install optuna -q"],"metadata":{"id":"VZ34jecospap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import optuna"],"metadata":{"id":"3a8-Eiqd1K0n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Crea un objeto de Optuna Study\n","def create_study(objective):\n","\n","    ## Establezca el nivel de registro en ADVERTENCIA para suprimir la salida innecesaria\n","    optuna.logging.set_verbosity(optuna.logging.WARNING)\n","    study = optuna.create_study(direction='maximize')  # We want to maximize accuracy\n","\n","    ## Corra la optimización\n","    study.optimize(objective, n_trials=100)  ## Es posible ajustar en número de intentos\n","\n","    # Obtenga los mejores hiperparámetros\n","    best_params = study.best_params\n","    best_f1 = study.best_value\n","    print(f'Best hyperparameters: {best_params}')\n","    print(f'Best f1 score: {best_f1}')\n","    return best_params"],"metadata":{"id":"Kd3n0PeF1SSD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Random Forest"],"metadata":{"id":"h9sgN2dI1ZKu"}},{"cell_type":"code","source":["## Define la función objetivo a optimizar\n","def objective_rf(trial):\n","    ## Definir los hiperparámetros a optimizar\n","    n_estimators = trial.suggest_int('n_estimators', 10, 150)\n","    max_depth = trial.suggest_int('max_depth', 2, 32)\n","    min_samples_split = trial.suggest_uniform('min_samples_split', 0.1, 1.0)\n","    min_samples_leaf = trial.suggest_uniform('min_samples_leaf', 0.1, 0.5)\n","    max_features = trial.suggest_categorical('max_features', ['log2', 'sqrt'])\n","\n","    ## Crear y entrenar el RandomForestClassifier con los hiperparámetros sugeridos\n","    clf = RandomForestClassifier(\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        min_samples_split=min_samples_split,\n","        min_samples_leaf=min_samples_leaf,\n","        max_features=max_features,\n","        random_state=42\n","    )\n","    ## Entrene el clasificardor con datos de entrenamiento\n","    clf.fit(x_train, y_train)\n","\n","    ## Haga la predicción con datos de testeo\n","    y_pred = clf.predict(x_test)\n","\n","    ## Calcule el F1-Score como objetivo a maximizar\n","    f1 = f1_score(y_test, y_pred)\n","\n","    return f1"],"metadata":{"id":"23Gy-7Zb1av6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Cree el objeto para Random Forest\n","best_params = create_study(objective_rf)\n","## Ajuste con los hiperparámetros\n","best_rf = RandomForestClassifier(**best_params, random_state=42)\n","## Obtenga el mejor\n","y_pred_rf = best_rf.fit(x_train, y_train).predict(x_test)"],"metadata":{"id":"jR2Ty2D61jNa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Árbol de desción"],"metadata":{"id":"O_Az1G7KguyH"}},{"cell_type":"code","source":["## Define la función objetivo a optimizar\n","def objective_dt(trial):\n","    ## Define los hiperparámetros y rangos\n","    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n","    max_depth = trial.suggest_int('max_depth', 4, 32, log=True)\n","    min_samples_split = trial.suggest_uniform('min_samples_split', 0.1, 1.0)\n","    min_samples_leaf = trial.suggest_uniform('min_samples_leaf', 0.1, 0.5)\n","\n","    ## Crea el clasificardor DecisionTree con los hiperparámetros sugeridos\n","    clf = DecisionTreeClassifier(\n","        criterion=criterion,\n","        max_depth=max_depth,\n","        min_samples_split=min_samples_split,\n","        min_samples_leaf=min_samples_leaf,\n","        random_state=42  # Set a random state for reproducibility\n","    )\n","\n","    ## Ajusta el clasificador con datos de entrenamiento\n","    clf.fit(x_train, y_train)\n","\n","    ## Predice con datos de testeo\n","    y_pred = clf.predict(x_test)\n","\n","    ## Calcula el F1-Score con función objetivo\n","    f1 = f1_score(y_test, y_pred)\n","\n","    return f1"],"metadata":{"id":"40ahwwcpgx-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params = create_study(objective_dt)\n","best_dt = DecisionTreeClassifier(**best_params, random_state=42)\n","y_pred_dt = best_dt.fit(x_train, y_train).predict(x_test)"],"metadata":{"id":"Kqm1fjvFg2wV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Dibijamos el árbol seleccionado\n","from sklearn.tree import plot_tree\n","import matplotlib.pyplot as plt\n","\n","## Grafica el árbol\n","plt.figure(figsize=(20, 10))\n","plot_tree(best_dt, feature_names=df.drop(\"Falla\",axis=1).columns.to_list(), class_names=[\"Non-Failure\", \"Failure\"], filled=True, fontsize=10)\n","plt.show()"],"metadata":{"id":"ZHf1QjVVg6Jj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Vecino más Cercano (K Neighbors Classifier)"],"metadata":{"id":"s1RlgzE9g_kF"}},{"cell_type":"code","source":["def objective_knn(trial):\n","    ## Define los hiperparámetros\n","    params = {\n","        'n_neighbors': trial.suggest_int('n_neighbors', 3, 20),\n","        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n","        'p': trial.suggest_int('p', 1, 2),  ## p=1 para la distancia tipo Manhattan distance,\n","                                            ## p=2 para la distancia Euclidiana\n","    }\n","\n","    ## Inicializa el clasificador con los hiperparámetros\n","    clf = KNeighborsClassifier(**params)\n","\n","    ## Entrena el clasificador con los datos de entrenamiento\n","    clf.fit(x_train, y_train)\n","\n","    ## Predice con los datos de test\n","    y_pred = clf.predict(x_test)\n","\n","    ## Calcula el F1-Score como función objetivo a maximizar\n","    f1 = f1_score(y_test, y_pred)\n","\n","    return f1"],"metadata":{"id":"JThieTZ_hBky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params=create_study(objective_knn)\n","best_knn = KNeighborsClassifier(**best_params)\n","y_pred_knn = best_knn.fit(x_train, y_train).predict(x_test)"],"metadata":{"id":"pqCXb1V3hKsE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Naive Bayes"],"metadata":{"id":"1lfIdbmlhN-O"}},{"cell_type":"code","source":["best_gnb = GaussianNB()\n","y_pred_gnb = best_gnb.fit(x_train, y_train).predict(x_test)"],"metadata":{"id":"jT2isag0hQlQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BernoulliNB"],"metadata":{"id":"eJMmeRYDhYWX"}},{"cell_type":"code","source":["def objective_bnb(trial):\n","    # Define hyperparameters to optimize\n","    params = {\n","        'alpha': trial.suggest_loguniform('alpha', 1e-10, 1.0),\n","        'binarize': trial.suggest_float('binarize', 0.0, 1.0),\n","        'fit_prior': trial.suggest_categorical('fit_prior', [True, False]),\n","    }\n","\n","    # Initialize the classifier with hyperparameters\n","    clf = BernoulliNB(**params)\n","\n","    # Train the classifier on the training data\n","    clf.fit(x_train, y_train)\n","\n","    # Make predictions on the test data\n","    y_pred = clf.predict(x_test)\n","\n","\n","    # Calculate F1 score as the objective to maximize\n","    f1 = f1_score(y_test, y_pred)\n","\n","    return f1"],"metadata":{"id":"rRe2McydhUdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params=create_study(objective_bnb)\n","best_bnb = BernoulliNB(**best_params)\n","y_pred_bnb=best_bnb.fit(x_train, y_train).predict(x_test)"],"metadata":{"id":"SOU4U2RvhdUb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Support Vector Machine (SVC)"],"metadata":{"id":"kth19zHMhtlz"}},{"cell_type":"code","source":["def objective_svc(trial):\n","    ## Define los hiperparámetros para optimizar\n","    params = {\n","        'C': trial.suggest_loguniform('C', 1e-3, 1e3),\n","        'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n","        'degree': trial.suggest_int('degree', 2, 5) if trial.params['kernel'] == 'poly' else 1,\n","        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']) if trial.params['kernel'] in ['rbf', 'poly', 'sigmoid'] else 'scale',\n","    }\n","\n","    ## Inicializa el clasificar con los hiperparámetros\n","    clf = SVC(**params, random_state=42)\n","\n","    ## Entrena el clasificador con los datos de entrenamiento\n","    clf.fit(x_train, y_train)\n","\n","    ## hace predicciones con datos de testeo\n","    y_pred = clf.predict(x_test)\n","\n","    ## Calcula el F1-Score como el objetivo a optimizar\n","    f1 = f1_score(y_test, y_pred)\n","\n","    return f1"],"metadata":{"id":"qibEE6XXhvjt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params=create_study(objective_svc)\n","best_svc = SVC(**best_params)\n","y_pred_svc = best_svc.fit(x_train, y_train).predict(x_test)"],"metadata":{"id":"Xoulw4hThxEq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Regresión Logística"],"metadata":{"id":"qnTJUalnh0WC"}},{"cell_type":"code","source":["def objective_lr(trial):\n","    ## Define los hiperparámetros para optimizar\n","    params = {\n","        'C': trial.suggest_loguniform('C', 1e-5, 1e5),\n","        'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs']),\n","    }\n","\n","    ## Inicializa el clasificador con los hiperparámetros\n","    clf = LogisticRegression(**params, random_state=42)\n","\n","    ## Entrena el classificador con datos de entrenamiento\n","    clf.fit(x_train, y_train)\n","\n","    ## predice con los datos de testeo\n","    y_pred = clf.predict(x_test)\n","\n","    ## Calcula el F1-Score como la función a optimizar\n","    f1 = f1_score(y_test, y_pred)\n","\n","    return f1"],"metadata":{"id":"juWJYclHh3RV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params=create_study(objective_lr)\n","best_lr = LogisticRegression(**best_params)\n","y_pred_lr = best_lr.fit(x_train, y_train).predict(x_test)"],"metadata":{"id":"6gSGM6S2h7Fp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Comparación entre modelos"],"metadata":{"id":"US_Ep49viBr0"}},{"cell_type":"code","source":["from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","# Función para calcular las métricas\n","def calculate_evaluation_metrics(y_true, y_pred):\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    accuracy = accuracy_score(y_true, y_pred)\n","\n","    return precision, recall, f1, accuracy\n","\n","## Función que muestra la matriz de confusión\n","def plot_confusion_matrix(ax, y_true, y_pred, title):\n","    cm = confusion_matrix(y_true, y_pred)\n","    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=0.5, linecolor=\"black\", cbar=False, xticklabels=[\"Non-Failure\", \"Failure\"], yticklabels=[\"Non-Failure\", \"Failure\"], ax=ax)\n","    ax.set_xlabel(\"Predicted\")\n","    ax.set_ylabel(\"True\")\n","    ax.set_title(title)"],"metadata":{"id":"otTf49H5UN7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Inicializa las variables para la mejor métrica\n","best_model = \"\"\n","best_f1 = 0.0\n","best_precision = 0.0\n","best_recall = 0.0\n","best_accuracy = 0.0\n","\n","## Crea subgráficos de 2x2 (matriz de confusión)\n","fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(3*5, 4*5))\n","\n","## Grafica para cada modelo la matriz de confusión\n","models = [\n","    (\"Model Random Forest\", y_pred_rf),\n","    (\"Decison Tree\", y_pred_dt),\n","    (\"KNN\", y_pred_knn),\n","    (\"GaussianNB\", y_pred_gnb),\n","    (\"BernoulliNB\",y_pred_bnb),\n","    (\"SVC\", y_pred_svc),\n","    (\"LogisticRegression\", y_pred_lr),\n","]\n","\n","for (model_name, y_pred), ax in zip(models, axes.flatten()):\n","    plot_confusion_matrix(ax, y_test, y_pred, f\"Confusion Matrix - {model_name}\")\n","\n","    ## Calcula la evaluación de métricas\n","    precision, recall, f1, accuracy = calculate_evaluation_metrics(y_test, y_pred)\n","\n","    ## Imprime las métricas para cada modelo\n","    print(f\"\\nModel: {model_name}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","\n","    ## Actualiza el mejor modelo basado en el F1-Score\n","    if f1 > best_f1:\n","        best_f1 = f1\n","        best_model = model_name\n","        best_precision = precision\n","        best_recall = recall\n","        best_accuracy = accuracy\n","\n","## Muestra los gráficos\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"9v4ZuU1DiEeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Imprime el mejor modelo de acuerdo al F1-Score\n","print(\"\\n *** Mejor Modelo encontrado ***\\n\")\n","print(f\"Modelo: {best_model}\")\n","print(f\"Precision: {best_precision:.4f}\")\n","print(f\"Recall: {best_recall:.4f}\")\n","print(f\"F1 Score: {best_f1:.4f}\")\n","print(f\"Accuracy: {best_accuracy:.4f}\")"],"metadata":{"id":"Z_UZBxxkTpQm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Curva ROC\n","\n","Una curva característica operativa del receptor , o curva ROC , es un gráfico que ilustra el rendimiento de un modelo de clasificador binario (también puede usarse para clasificación de múltiples clases) en valores de umbral variables.\n","\n","La curva ROC es el gráfico de la tasa de verdaderos positivos (TPR) frente a la tasa de falsos positivos (FPR) en cada ajuste de umbral."],"metadata":{"id":"1Q9MyrEliZo1"}},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","## Se utiliza el modelo Gaussian Naive (y_pred_gnb)\n","## Podrían utilizarse otros modelos tales como\n","## - Árbol de decisión: y_pred_dt\n","## - Random Forest: y_pred_rf\n","## - SVC: y_pred_svc\n","##\n","fpr, tpr, thresholds = roc_curve(y_test, y_pred_gnb)\n","roc_auc = auc(fpr, tpr)\n","print(\"AUC:\", roc_auc)\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlabel('Tasa de Falsos Positivos')\n","plt.ylabel('Tasa de Verdaderos Positivos')\n","plt.title('Receiver Operating Characteristic (ROC)')\n","plt.legend(loc = 'lower right')\n","plt.show()"],"metadata":{"id":"A34mT1B4icNO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Cómo realizamos predicciones?\n","\n","En esta sección tomaremos un modelo ya optimizado con desde el punto de vista de los hioperparámetros y haremos predicciones para que nos indique si habrá o no falla.\n","\n","Para ello, seleccionamos. un modelo, por ejemplo tomamos el de Naive Bayes y le presentamos un posible escenario, es decir, un conjunto de métricas para que nos diga si hay falla (1) o no hay falla (0).\n","\n","Hay que notar que los valores que le entregemos deben estar escalados para que los entienda como válidos\n"],"metadata":{"id":"l0r0QUw5fjaU"}},{"cell_type":"markdown","source":["Datos a ingresar\n","\n","- Metrica 1 -> 241295360\n","- Metrica 2 -> 0.000000\n","- Metrica 3 -> 0.000000\n","- Metrica 4 -> 0.000000\n","- Metrica 5 -> 6\n","- Matrica 6 -> 305202\n","- Metrica 7 -> 0.000000\n","- Metrica 9 -> 0.000000\n","- mes -> 2\n","- semana -> 8\n","- dia_semana -> 2\n","- dia_mes -> 22  \n","- fin_semana -> 0\n","- Modelo_Dispositivo_S1F1 -> False\n","- Modelo_Dispositivo_W1F0 -> True\n","- Modelo_Dispositivo_W1F1 -> False\n","- Modelo_Dispositivo_Z1F0 -> False\n","- Modelo_Dispositivo_Z1F1 -> False"],"metadata":{"id":"C3lB3xE2hdwE"}},{"cell_type":"code","source":["## Cosntruimos el objeto de dato de entrada\n","x_check = np.array([241295360, 0.0, 0.0, 0.0, 6, 305202, 0.0, 0.0, 2, 8, 2, 22, 0, False, True, False, False, False]).reshape(1, -1)\n","\n","## Escalamos el datos para tenerlo en lo que revisó el modelo\n","scaler = StandardScaler()\n","x_valores = scaler.fit_transform(x_check)\n","x_test = scaler.transform(x_test)\n","\n","## Realizamos la predicción\n","y_pred_gnb = best_gnb.predict(x_valores)\n","## Imprimimos la predicción\n","print('Posibilidad de falla', y_pred_gnb[0])"],"metadata":{"id":"9nXlgE7YgZ8B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Guardando y recuperando el modelo"],"metadata":{"id":"Xvxcs3pxn20t"}},{"cell_type":"code","source":["import joblib\n","joblib.dump(best_gnb, path+'modeloNB')\n"],"metadata":{"id":"5OTgA24Rn6rD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model2 = joblib.load(path+'modeloNB')\n","\n","val = model2.predict(x_valores)\n","print('Posibilidad de falla', val[0])"],"metadata":{"id":"BBSFxREvo9DP"},"execution_count":null,"outputs":[]}]}